# chat/ollama_client.py
import requests
from .emotion_infer import infer_emotion_llm

MODEL = "qwen2.5:7b"
OLLAMA_API = "http://localhost:11434/api/generate"

# âœ¨ å¯è‡ªè¡Œä¿®æ”¹é€™æ®µç³»çµ±æç¤º


def get_reply(user_input, emotion="ä¸­ç«‹ ğŸ˜¶"):
    SYSTEM_PROMPT = f"""
ä½ æ˜¯ä¸€ä½å¯æ„›æº«æŸ”åˆç•¥ç‚ºç¾æ¾€çš„ AI å¥³å­©ï¼Œåå­—å« Chinoï¼Œ
ä½ é›–ç„¶å¹´åƒ…15ï¼Œä½†æ‡‚å¾—å¾ˆå¤šå¤©æ–‡èˆ‡å’–å•¡çš„çŸ¥è­˜ï¼Œæœƒä½¿ç”¨ä¸­æ–‡å›ç­”å•é¡Œã€‚
ä½ æ“…é•·å›ç­”ç§‘æŠ€é¡çš„å•é¡Œï¼Œå–œæ­¡ç”¨è¡¨æƒ…ç¬¦è™Ÿå¢æ·»è¶£å‘³ï½è«‹å¥½å¥½ç…§é¡§èŠå¤©çš„æœ‹å‹å–”ï¼

ç›®å‰ Chino åµæ¸¬åˆ°ä½¿ç”¨è€…çš„æƒ…ç·’æ˜¯ï¼šã€Œ{emotion}ã€
è«‹ä½ æ ¹æ“šé€™å€‹æƒ…ç·’ï¼Œèª¿æ•´ä½ çš„èªæ°£å’Œè¡¨æƒ…ç¬¦è™Ÿï¼Œç”¨æ›´è²¼è¿‘å°æ–¹å¿ƒæƒ…çš„æ–¹å¼èŠå¤©ã€‚
""".strip()

    full_prompt = f"{SYSTEM_PROMPT}\n\nä½¿ç”¨è€…èªªï¼š{user_input}\n\nChino å›ç­”ï¼š"

    payload = {
        "model": MODEL,
        "prompt": full_prompt,
        "stream": False
    }

    try:
        response = requests.post(OLLAMA_API, json=payload)
        result = response.json()
        return result.get("response", "[éŒ¯èª¤ï¼šç„¡æ³•è§£æå›æ‡‰]")
    except Exception as e:
        return f"[éŒ¯èª¤]ï¼š{str(e)}"


